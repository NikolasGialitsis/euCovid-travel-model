{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EU_Covid.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM5euRb+tPiD0wB0dAz3vah",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NikolasGialitsis/euCovid-travel-model/blob/master/EU_Covid.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYIYJUonUPWH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "182dd3e8-5044-4be1-c3cd-a477307ee101"
      },
      "source": [
        "!git clone https://github.com/NikolasGialitsis/euCovid-travel-model"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'euCovid-travel-model'...\n",
            "remote: Enumerating objects: 87, done.\u001b[K\n",
            "remote: Counting objects:   1% (1/87)\u001b[K\rremote: Counting objects:   2% (2/87)\u001b[K\rremote: Counting objects:   3% (3/87)\u001b[K\rremote: Counting objects:   4% (4/87)\u001b[K\rremote: Counting objects:   5% (5/87)\u001b[K\rremote: Counting objects:   6% (6/87)\u001b[K\rremote: Counting objects:   8% (7/87)\u001b[K\rremote: Counting objects:   9% (8/87)\u001b[K\rremote: Counting objects:  10% (9/87)\u001b[K\rremote: Counting objects:  11% (10/87)\u001b[K\rremote: Counting objects:  12% (11/87)\u001b[K\rremote: Counting objects:  13% (12/87)\u001b[K\rremote: Counting objects:  14% (13/87)\u001b[K\rremote: Counting objects:  16% (14/87)\u001b[K\rremote: Counting objects:  17% (15/87)\u001b[K\rremote: Counting objects:  18% (16/87)\u001b[K\rremote: Counting objects:  19% (17/87)\u001b[K\rremote: Counting objects:  20% (18/87)\u001b[K\rremote: Counting objects:  21% (19/87)\u001b[K\rremote: Counting objects:  22% (20/87)\u001b[K\rremote: Counting objects:  24% (21/87)\u001b[K\rremote: Counting objects:  25% (22/87)\u001b[K\rremote: Counting objects:  26% (23/87)\u001b[K\rremote: Counting objects:  27% (24/87)\u001b[K\rremote: Counting objects:  28% (25/87)\u001b[K\rremote: Counting objects:  29% (26/87)\u001b[K\rremote: Counting objects:  31% (27/87)\u001b[K\rremote: Counting objects:  32% (28/87)\u001b[K\rremote: Counting objects:  33% (29/87)\u001b[K\rremote: Counting objects:  34% (30/87)\u001b[K\rremote: Counting objects:  35% (31/87)\u001b[K\rremote: Counting objects:  36% (32/87)\u001b[K\rremote: Counting objects:  37% (33/87)\u001b[K\rremote: Counting objects:  39% (34/87)\u001b[K\rremote: Counting objects:  40% (35/87)\u001b[K\rremote: Counting objects:  41% (36/87)\u001b[K\rremote: Counting objects:  42% (37/87)\u001b[K\rremote: Counting objects:  43% (38/87)\u001b[K\rremote: Counting objects:  44% (39/87)\u001b[K\rremote: Counting objects:  45% (40/87)\u001b[K\rremote: Counting objects:  47% (41/87)\u001b[K\rremote: Counting objects:  48% (42/87)\u001b[K\rremote: Counting objects:  49% (43/87)\u001b[K\rremote: Counting objects:  50% (44/87)\u001b[K\rremote: Counting objects:  51% (45/87)\u001b[K\rremote: Counting objects:  52% (46/87)\u001b[K\rremote: Counting objects:  54% (47/87)\u001b[K\rremote: Counting objects:  55% (48/87)\u001b[K\rremote: Counting objects:  56% (49/87)\u001b[K\rremote: Counting objects:  57% (50/87)\u001b[K\rremote: Counting objects:  58% (51/87)\u001b[K\rremote: Counting objects:  59% (52/87)\u001b[K\rremote: Counting objects:  60% (53/87)\u001b[K\rremote: Counting objects:  62% (54/87)\u001b[K\rremote: Counting objects:  63% (55/87)\u001b[K\rremote: Counting objects:  64% (56/87)\u001b[K\rremote: Counting objects:  65% (57/87)\u001b[K\rremote: Counting objects:  66% (58/87)\u001b[K\rremote: Counting objects:  67% (59/87)\u001b[K\rremote: Counting objects:  68% (60/87)\u001b[K\rremote: Counting objects:  70% (61/87)\u001b[K\rremote: Counting objects:  71% (62/87)\u001b[K\rremote: Counting objects:  72% (63/87)\u001b[K\rremote: Counting objects:  73% (64/87)\u001b[K\rremote: Counting objects:  74% (65/87)\u001b[K\rremote: Counting objects:  75% (66/87)\u001b[K\rremote: Counting objects:  77% (67/87)\u001b[K\rremote: Counting objects:  78% (68/87)\u001b[K\rremote: Counting objects:  79% (69/87)\u001b[K\rremote: Counting objects:  80% (70/87)\u001b[K\rremote: Counting objects:  81% (71/87)\u001b[K\rremote: Counting objects:  82% (72/87)\u001b[K\rremote: Counting objects:  83% (73/87)\u001b[K\rremote: Counting objects:  85% (74/87)\u001b[K\rremote: Counting objects:  86% (75/87)\u001b[K\rremote: Counting objects:  87% (76/87)\u001b[K\rremote: Counting objects:  88% (77/87)\u001b[K\rremote: Counting objects:  89% (78/87)\u001b[K\rremote: Counting objects:  90% (79/87)\u001b[K\rremote: Counting objects:  91% (80/87)\u001b[K\rremote: Counting objects:  93% (81/87)\u001b[K\rremote: Counting objects:  94% (82/87)\u001b[K\rremote: Counting objects:  95% (83/87)\u001b[K\rremote: Counting objects:  96% (84/87)\u001b[K\rremote: Counting objects:  97% (85/87)\u001b[K\rremote: Counting objects:  98% (86/87)\u001b[K\rremote: Counting objects: 100% (87/87)\u001b[K\rremote: Counting objects: 100% (87/87), done.\u001b[K\n",
            "remote: Compressing objects: 100% (80/80), done.\u001b[K\n",
            "remote: Total 87 (delta 15), reused 69 (delta 7), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (87/87), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8GuZXzutmXTU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "0ffefed5-6491-4931-d7df-1d15b99994c1"
      },
      "source": [
        "!pip install python-copasi\n",
        "%cd /content/euCovid-travel-model/generating"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting python-copasi\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e3/c3/67d338d0869ebeca278171710c82d03365f4247e036fe329f26db4e5e225/python_copasi-4.28.226-cp36-cp36m-manylinux1_x86_64.whl (9.7MB)\n",
            "\u001b[K     |████████████████████████████████| 9.7MB 2.7MB/s \n",
            "\u001b[?25hInstalling collected packages: python-copasi\n",
            "Successfully installed python-copasi-4.28.226\n",
            "[Errno 2] No such file or directory: '/content/euCovid-travel-model/generating'\n",
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fbL6xOwwV29W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mv /content/euCovid-travel-model/EU_expand_model.py /content/euCovid-travel-model/generating"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4F9BjYJnB8s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "0fcff0b6-9b2c-4199-e2f9-a98802568813"
      },
      "source": [
        "!python /content/euCovid-travel-model/generating/EU_expand_model.py /content/euCovid-travel-model/generating/SEIR_base.cps eu_usa.cps"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "abrev: {'GR': 0, 'IT': 1, 'BU': 2}\n",
            "tcmalloc: large alloc 1449377792 bytes == 0x1e54000 @  0x7f75c2b22001 0x582743 0x582af3 0x50d164 0x507d64 0x509a90 0x50a48d 0x50bfb4 0x507d64 0x50ae13 0x634c82 0x634d37 0x6384ef 0x639091 0x4b0d00 0x7f75c271db97 0x5b250a\n",
            "compartment name =  COVID_GR\n",
            "state_abrev =  GR\n",
            "GR 0\n",
            "state flux =  [{0: 0.1, 1: 0.005, 2: 0.01}, {0: 0.1, 1: 0.0001, 2: 0.002}]\n",
            "compartment name =  COVID_IT\n",
            "state_abrev =  IT\n",
            "IT 1\n",
            "state flux =  [{0: 0.0001, 1: 0.02, 2: 0.0025}, {0: 0.005, 1: 0.02, 2: 0.0005}]\n",
            "compartment name =  COVID_BU\n",
            "state_abrev =  BU\n",
            "BU 2\n",
            "state flux =  [{0: 0.002, 1: 0.0005, 2: 0.006666666666666667}, {0: 0.01, 1: 0.0025, 2: 0.006666666666666667}]\n",
            "Number of assigments = 12\n",
            "\n",
            "Number of assigments = 12\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qvtEpCQ3g1wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "e03a2028-3610-4552-b6a3-d4cf22943ad0"
      },
      "source": [
        "%cd /content/\n",
        "!kaggle datasets download -d shahules/european-countries-population"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 146, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "IOError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5Y4ajr_gBz4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "6059d59f-441f-4f3f-afea-48c3591d0e31"
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import sys\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import datasets, linear_model\n",
        "from random import sample \n",
        "from google.colab import drive\n",
        "\n",
        "def bash_collect_data():\n",
        "  sys.path.append('/usr/local/lib/python3.7/site-packages/')\n",
        "  drive.mount('/content/drive')\n",
        "  os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/My Drive/Colab Notebooks/.kaggle/\"\n",
        "  !kaggle datasets download -d datagraver/eurovision-song-contest-scores-19752019\n",
        "  !kaggle datasets download -d sudalairajkumar/novel-corona-virus-2019-dataset\n",
        "  !kaggle datasets download -d duvallwh/us-asia-and-europe-population-and-area-data\n",
        "  !unzip /content/european-countries-population.zip\n",
        "  !unzip /content/novel-corona-virus-2019-dataset.zip\n",
        "  !unzip /content/us-asia-and-europe-population-and-area-data.zip\n",
        "  !unzip datasets download -d datagraver/eurovision-song-contest-scores-19752019\n",
        "\n",
        "def PopsAreas2dict(fn='/content/area_pop_data.csv'):\n",
        "    df = pd.read_csv(fn)\n",
        "    df = df[df['region'] == 'Europe']\n",
        "    print(df.columns)\n",
        "    areas = {}\n",
        "    pops = {}\n",
        "    for row in df.iterrows():\n",
        "      name = row[1]['country'].upper()\n",
        "      name = name.split(' ')[0]\n",
        "      area = row[1]['area_km']\n",
        "      pop = row[1]['population']\n",
        "      if name not in areas.keys():\n",
        "        areas[name] = area\n",
        "        pops[name] = pop\n",
        "      else:\n",
        "        areas[name] += area \n",
        "        pops[name] += pop\n",
        "    return pops,areas\n",
        "\n",
        "def CovidCountry(fn,countries):\n",
        "  df = pd.read_csv(fn)\n",
        "  df = df.drop(labels=['Lat','Long'],axis='columns')\n",
        "  df['lastmonth'] = df[df.columns[-30:]].sum(axis=1) \n",
        "  infected = {}\n",
        "  for row in df.iterrows():\n",
        "    name = row[1]['Country/Region'].upper()\n",
        "    name = name.split(' ')[0]\n",
        "    confirmed = row[1]['lastmonth']\n",
        "    if name not in countries:\n",
        "      continue\n",
        "    if name not in infected.keys():\n",
        "      infected[name] = confirmed\n",
        "    else:\n",
        "      infected[name] += confirmed\n",
        "  return infected\n",
        "\n",
        "def getEurovision(fn,countries):\n",
        "  df = pd.read_excel(fn) \n",
        "  FromDict = {}\n",
        "  ToDict = {}\n",
        "  \n",
        "  for row in df.iterrows():\n",
        "    n_from = row[1]['From country'].upper()\n",
        "    n_to = row[1]['To country'].upper()\n",
        "    if n_from not in countries or n_to not in countries:\n",
        "      continue\n",
        "    points =  row[1]['Points      ']\n",
        "    if n_from not in FromDict.keys():\n",
        "      FromDict[n_from] = {n_to:points}\n",
        "    else:\n",
        "      if n_to in FromDict[n_from].keys():\n",
        "        FromDict[n_from][n_to] += points\n",
        "      else:\n",
        "        FromDict[n_from][n_to] = points\n",
        "\n",
        "    if n_to not in ToDict.keys():\n",
        "      ToDict[n_to] = {n_from:points}\n",
        "    else:\n",
        "      if n_from in ToDict[n_to].keys():\n",
        "        ToDict[n_to][n_from] += points\n",
        "      else:\n",
        "        ToDict[n_to][n_from] = points \n",
        "  for key in FromDict:\n",
        "    total = sum([x for x in FromDict[key].values()])\n",
        "    FromDict[key] = {k: v / total for k, v in FromDict[key].items()}\n",
        "  for key in ToDict:\n",
        "    total = sum([x for x in ToDict[key].values()])\n",
        "    ToDict[key] = {k: v / total for k, v in ToDict[key].items()}\n",
        "  \n",
        "  \n",
        "  for country in EUcountries:\n",
        "    if country not in FromDict.keys():\n",
        "      FromDict[country] = {}\n",
        "    if country not in ToDict.keys():\n",
        "      ToDict[country] = {}\n",
        "\n",
        "  for source in FromDict.keys():\n",
        "    for target in EUcountries:\n",
        "      if target not in FromDict[source].keys():\n",
        "        FromDict[source][target] = 0.0\n",
        "  for source in ToDict.keys():\n",
        "    for target in EUcountries:\n",
        "      if target not in ToDict[source].keys():\n",
        "        ToDict[source][target] = 0.0 \n",
        "      \n",
        "  return FromDict,ToDict\n",
        "\n",
        "def getFluxes(FromDict,ToDict,Pops):\n",
        "  Fluxes = {}\n",
        "  for country in FromDict.keys():\n",
        "    ToDict[country] = {k: v * Pops[country] for k, v in ToDict[country].items()}\n",
        "    FromDict[country] = {k: v * Pops[country] for k, v in FromDict[country].items()}\n",
        "    Fluxes[country] = [FromDict[country],ToDict[country]]\n",
        "  return Fluxes\n",
        "\n",
        "\n",
        "#bash_collect_data()\n",
        "#!unzip /content/eurovision-song-contest-scores-19752019.zip\n",
        "pops,areas =  PopsAreas2dict('/content/area_pop_data.csv')\n",
        "\n",
        "EUcountries = pops.keys()\n",
        "print(pops.keys())\n",
        "infected = CovidCountry(fn='/content/time_series_covid_19_confirmed.csv',countries=EUcountries)\n",
        "FromDict,ToDict = getEurovision('/content/eurovision_song_contest_1975_2019.xlsx',EUcountries)\n",
        "Fluxes = getFluxes(FromDict,ToDict,pops)\n",
        "\n",
        "\n",
        "    "
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Unnamed: 0', 'region', 'country', 'province', 'area_km', 'population'], dtype='object')\n",
            "dict_keys(['ALBANIA', 'ANDORRA', 'ARMENIA', 'AUSTRIA', 'AZERBAIJAN', 'BELARUS', 'BELGIUM', 'BOSNIA', 'BULGARIA', 'CROATIA', 'CYPRUS', 'CZECH', 'DENMARK', 'ESTONIA', 'FAROE', 'FINLAND', 'FRANCE', 'GEORGIA', 'GERMANY', 'GIBRALTAR', 'GREECE', 'GUERNSEY', 'HUNGARY', 'ICELAND', 'REPUBLIC', 'ISLE', 'ITALY', 'JERSEY', 'KAZAKHSTAN', 'KOSOVO', 'LATVIA', 'LIECHTENSTEIN', 'LITHUANIA', 'LUXEMBOURG', 'MALTA', 'MOLDOVA', 'MONACO', 'MONTENEGRO', 'NETHERLANDS', 'NORTH', 'NORWAY', 'POLAND', 'PORTUGAL', 'ROMANIA', 'RUSSIA', 'SAN', 'SERBIA', 'SLOVAKIA', 'SLOVENIA', 'SPAIN', 'MAYEN', 'SWEDEN', 'SWITZERLAND', 'TRANSNISTRIA', 'TURKEY', 'UKRAINE', 'UNITED', 'VATICAN', 'ÅLAND'])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqBlb-4UFrOY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  \n",
        "  #US_states = make_model.process_travel_data.stateKeysLoad('travel_data/state_county_FIPS.csv')\n",
        "  US_states = {0:'GR',1:'IT',2:'BU'}\n",
        "  id = 0\n",
        "  US_states = {}\n",
        "  for country in EUcountries:\n",
        "    US_states[country] = country\n",
        "\n",
        "\n",
        "  #US_state_populations = make_model.process_travel_data.statePopsLoad('travel_data/countypops_2013.csv')\n",
        "  US_state_populations = pops\n",
        "\n",
        "  #US_state_land_areas = make_model.process_travel_data.stateAreasLoad('state_land_area.txt')\n",
        "  US_state_land_areas = areas\n",
        "  \n",
        "  # Only model states which have population data and land area\n",
        "  #US_states = {key: value for (key, value) in US_states.items() if key in US_state_populations and value in US_state_land_areas}\n",
        "\n",
        "  # Get the reverse mapping\n",
        "  state_abrev_to_key = {value: value for value in EUcountries}\n",
        "  # Get the state-centric flows in and out\n",
        "  #state_fluxes = make_model.process_travel_data.makeStateFluxes('travel_data/commute_flow_counts.csv')\n",
        "  \n",
        "  #[IN,OUT]\n",
        "\n",
        "  state_fluxes = Fluxes.copy()"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGpgkTnxJds5",
        "colab_type": "text"
      },
      "source": [
        "!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgdxOWe9iLaA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "4df7e8c1-7d74-4e8c-df46-e7527b09657f"
      },
      "source": [
        "#!/usr/bin/env python\n",
        "\n",
        "import COPASI\n",
        "#import make_model.process_travel_data\n",
        "import sys\n",
        "\n",
        "def expand_model(input_file_path, output_file_path):\n",
        "  \"\"\"expands the model in the given file.\"\"\"\n",
        "\n",
        "  assert COPASI.CRootContainer.getRoot() != None\n",
        "  # create a new datamodel\n",
        "  dataModel = COPASI.CRootContainer.addDatamodel()\n",
        "  assert COPASI.CRootContainer.getDatamodelList().size() == 1\n",
        "  \n",
        "  # load the base model\n",
        "  dataModel.loadModel(input_file_path)\n",
        "  \n",
        "  # get the model from the datamodel\n",
        "  model = dataModel.getModel()\n",
        "  assert model != None\n",
        "  \n",
        "  modelName = 'SEIR State Travel'\n",
        "  \n",
        "  model.setObjectName(modelName)\n",
        "  \n",
        "\n",
        "  home_to_work_event = model.createEvent('home to work event')\n",
        "  home_to_work_trigger_expression = 'sin(2*pi*<CN=Root,Model={0},Reference=Time>) > 0'.format(modelName)\n",
        "  home_to_work_event.setTriggerExpression(home_to_work_trigger_expression)\n",
        "  work_to_home_event = model.createEvent('work to home event')\n",
        "  work_to_home_trigger_expression = 'sin(2*pi*(<CN=Root,Model={0},Reference=Time>+0.34)) > 0'.format(modelName)\n",
        "  work_to_home_event.setTriggerExpression(work_to_home_trigger_expression)\n",
        "  \n",
        "  \n",
        "  modelElementsToExpand = COPASI.CModelExpansion_SetOfModelElements()\n",
        "  compartmentToReplicate = model.getCompartment(0)\n",
        "  \n",
        "  compartmentToReplicate.setObjectName('COVID')\n",
        "  \n",
        "  metabsToReplicate = {metab.getObjectName(): metab for metab in model.getMetabolites()}\n",
        "  \n",
        "  reactionsToReplicate = [reac for reac in model.getReactions()]\n",
        "  \n",
        "  flow_weight_template = '({0}*<{1}>/<{2}>)'\n",
        "  \n",
        "  total_adj_flow_template = '({0})/{1}'\n",
        "  \n",
        "  # Map of compartment species to CN, for use in event assignment creation\n",
        "  comp_metab_CN = dict()\n",
        "  \n",
        "  # Make something to iterate over for event assignments, which with also have US_state_keys\n",
        "  compartment_names_new = dict()\n",
        "  \n",
        "  # Map for the state keys to the new compartment names\n",
        "  US_state_key_to_comp = dict()\n",
        "  \n",
        "  # Create all the replicate compartments (and species and reactions)\n",
        "  for US_state_key, US_state_abrev in US_states.items():\n",
        "  \n",
        "      # need an empty elements map, each time\n",
        "      elementsMap = COPASI.CModelExpansion_ElementsMap()\n",
        "      modelElementsToExpand.addCompartment(compartmentToReplicate)\n",
        "  \n",
        "      for metab in metabsToReplicate.values():\n",
        "          modelElementsToExpand.addMetab(metab)\n",
        "          \n",
        "      for reaction in reactionsToReplicate:\n",
        "          modelElementsToExpand.addReaction(reaction)\n",
        "  \n",
        "      modelElementsToExpand.fillDependencies(model)\n",
        "      expansion = COPASI.CModelExpansion(model)\n",
        "      expansion.duplicate(modelElementsToExpand, '_' + US_state_abrev, elementsMap)\n",
        "  \n",
        "      # Get the new metabs, for use below\n",
        "      metabs = {name: elementsMap.getDuplicateFromObject(metabsToReplicate[name]) for name in metabsToReplicate.keys()}\n",
        "  \n",
        "      # change the particle numbers for the copied 'S' and 'N', to the given US state's population\n",
        "      metabs['S'].setInitialValue(US_state_populations[US_state_key])\n",
        "      metabs['N'].setInitialValue(US_state_populations[US_state_key])\n",
        "  \n",
        "      # set the compartment area to that of the state land area\n",
        "      this_state_area = US_state_land_areas[US_state_abrev]\n",
        "      elementsMap.getDuplicateFromObject(compartmentToReplicate).setInitialValue(this_state_area)\n",
        "  \n",
        "      # Add this (keyed) compartment name, for use in building event assignments\n",
        "      compartment_name_new = elementsMap.getDuplicateFromObject(compartmentToReplicate).getObjectName()\n",
        "      compartment_names_new[US_state_key] = compartment_name_new\n",
        "  \n",
        "      # Add the new CNs to the compartment+metab to ValueReference CN mapping\n",
        "      for metab_name, metab in metabsToReplicate.items():\n",
        "          comp_metab_CN[(compartment_name_new, metab_name)] = elementsMap.getDuplicateFromObject(metab).getValueReference().getCN()\n",
        "  \n",
        "  # Build the event assignments\n",
        "  for compartment_name in compartment_names_new.values():\n",
        "  \n",
        "      # Need the actual compartment metabs, because the metab concentration, not the particle number, needs to be the target\n",
        "      compartment = model.getCompartments().getByName(compartment_name)\n",
        "      compartment_metabs = compartment.getMetabolites()\n",
        "  \n",
        "      # Grab the land area, for use in creating the target as a concentration\n",
        "      land_area = compartment.getInitialValue()\n",
        "  \n",
        "      # Work-around to match compartment names generated by CModelExpansionDuplicate to ones pulled in from the US_states\n",
        "      \n",
        "      state_abrev = compartment_name.replace(compartmentToReplicate.getObjectName() + '_', '') \n",
        "      print('compartment name = ',compartment_name)\n",
        "      # Load the data about how many move in and out\n",
        "      print('state_abrev = ',state_abrev)\n",
        "      state_flux = state_fluxes[state_abrev_to_key[state_abrev]]\n",
        "      \n",
        "      print(state_abrev,state_abrev_to_key[state_abrev])\n",
        "      print('state flux = ',state_flux)\n",
        "      # Create assignment objects for all but the 'N' species\n",
        "      #    event_assignments = {metab_name: movement_event.createAssignment() for metab_name in metabsToReplicate.keys() if metab_name is not 'N'}\n",
        "  \n",
        "  \n",
        "  \n",
        "      N_val_CN = comp_metab_CN[(compartment_name, 'N')]\n",
        "  \n",
        "      for metab_name in metabsToReplicate.keys():\n",
        "          if metab_name is 'N':\n",
        "              continue\n",
        "          metab = compartment_metabs.getByName(metab_name)\n",
        "          metab_val_CN = comp_metab_CN[(compartment_name, metab_name)]\n",
        "      \n",
        "          # Morning commute event flows\n",
        "          # Build the list of morning event in-flow terms to string join\n",
        "          inTermListMorning = list()\n",
        "          inTermListMorning.append('<' + str(metab_val_CN) + '>') # particles before change\n",
        "          for state_key, inFlow in state_flux[0].items():\n",
        "              name_compartment_from = compartment_names_new[state_key]\n",
        "              this_N_val_CN = comp_metab_CN[(name_compartment_from, 'N')]\n",
        "              this_metab_val_CN = comp_metab_CN[(name_compartment_from, metab_name)]\n",
        "              inTerm = flow_weight_template.format(str(inFlow), str(this_metab_val_CN),str(this_N_val_CN))\n",
        "              inTermListMorning.append(inTerm)\n",
        "  \n",
        "\n",
        "          # Build the list of morning event out-flow terms to string join\n",
        "          outTermListMorning = list()\n",
        "          for state_key, outFlow in state_flux[1].items():\n",
        "              outTerm = flow_weight_template.format(str(outFlow), str(metab_val_CN),str(N_val_CN))\n",
        "              outTermListMorning.append(outTerm)\n",
        "  \n",
        "\n",
        "          flow_terms_morning = '+'.join(inTermListMorning) + '-' + '-'.join(outTermListMorning)\n",
        "          full_expression_morning = total_adj_flow_template.format(flow_terms_morning, land_area)\n",
        " \n",
        "          assignmentMorning = home_to_work_event.createAssignment()\n",
        "          assignmentMorning.setTargetCN(metab.getCN())\n",
        "          assignmentMorning.setExpression(full_expression_morning)\n",
        "\n",
        "          # Evening return from work event flows\n",
        "          # Build the list of evening event in-flow terms to string join\n",
        "          inTermListEvening = list()\n",
        "          inTermListEvening.append('<' + str(metab_val_CN) + '>') # particles before change\n",
        "          for state_key, inFlow in state_flux[1].items():\n",
        "              name_compartment_from = compartment_names_new[state_key]\n",
        "              this_N_val_CN = comp_metab_CN[(name_compartment_from, 'N')]\n",
        "              this_metab_val_CN = comp_metab_CN[(name_compartment_from, metab_name)]\n",
        "              inTerm = flow_weight_template.format(str(inFlow), str(this_metab_val_CN),str(this_N_val_CN))\n",
        "              inTermListEvening.append(inTerm)\n",
        "  \n",
        "\n",
        "          # Build the list of evening event out-flow terms to string join\n",
        "          outTermListEvening = list()\n",
        "          for state_key, outFlow in state_flux[0].items():\n",
        "              outTerm = flow_weight_template.format(str(outFlow), str(metab_val_CN),str(N_val_CN))\n",
        "              outTermListEvening.append(outTerm)\n",
        "  \n",
        "\n",
        "          flow_terms_evening = '+'.join(inTermListEvening) + '-' + '-'.join(outTermListEvening)\n",
        "          full_expression_evening = total_adj_flow_template.format(flow_terms_evening, land_area)\n",
        " \n",
        "          assignmentEvening = work_to_home_event.createAssignment()\n",
        "          assignmentEvening.setTargetCN(metab.getCN())\n",
        "          assignmentEvening.setExpression(full_expression_evening)\n",
        "  \n",
        "  print('Number of assigments = ' + str(home_to_work_event.getAssignments().size()) + '\\n')\n",
        "  print('Number of assigments = ' + str(work_to_home_event.getAssignments().size()) + '\\n')\n",
        "  \n",
        "  model.forceCompile()\n",
        "  \n",
        "  dataModel.saveModel(output_file_path, True)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\": \n",
        "  input = '/content/euCovid-travel-model/generating/SEIR_base.cps'\n",
        "  output = 'eurocovid2020.cps'\n",
        "  expand_model(input,output)\n",
        "\n",
        "main()\n"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-104-a491c0cdb6a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/euCovid-travel-model/generating/SEIR_base.cps'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m   \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'eurocovid2020.cps'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m   \u001b[0mexpand_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-104-a491c0cdb6a4>\u001b[0m in \u001b[0;36mexpand_model\u001b[0;34m(input_file_path, output_file_path)\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0;31m# create a new datamodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0mdataModel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCOPASI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRootContainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddDatamodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0;32massert\u001b[0m \u001b[0mCOPASI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCRootContainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetDatamodelList\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m# load the base model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: "
          ]
        }
      ]
    }
  ]
}